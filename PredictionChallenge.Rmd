---
title: "PredictionChallenge"
output: html_document
date: "2024-02-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
Sys.setenv(R_MAX_PPSIZE='500000')
```

```{r}
prediction_df <- read.csv("predictionChallenge.csv", header = TRUE)
attractions_df <- read.csv("Attractions.csv", header = TRUE)

head(prediction_df)
head(attractions_df)
```

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Updated function to calculate the minimum distance, including NA handling
calculate_min_distance <- function(lat1, lon1, attractions_df) {
  if (is.na(lat1) || is.na(lon1)) {
    return(NA)  # Return NA if the listing's latitude or longitude is NA
  }
  
  # Use 'Latitude' and 'Longitude' for attractions_df
  distances <- sqrt((lat1 - attractions_df$Latitude)^2 + (lon1 - attractions_df$Longitude)^2)
  valid_distances <- distances[!is.na(distances)]
  
  if (length(valid_distances) == 0) {  # If all distances are NA or no valid distances
    return(NA)
  } else {
    return(min(valid_distances))
  }
}

# Apply the function to calculate distances
prediction_df$distance_to_nearest_attraction <- mapply(calculate_min_distance,
                                                       lat1 = prediction_df$latitude,  # Ensure this matches your prediction_df column names
                                                       lon1 = prediction_df$longitude,  # Ensure this matches your prediction_df column names
                                                       MoreArgs = list(attractions_df = attractions_df))
```

```{r}
ggplot(prediction_df, aes(x = Deal.Quality, y = distance_to_nearest_attraction, fill = Deal.Quality)) +
  geom_violin(trim = FALSE) +
  labs(title = "Distance to Nearest Attraction by Deal Quality", x = "Deal Quality", y = "Distance to Nearest Attraction") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
# Assuming "Price..." is your actual price column, you might need to rename or directly reference it
ggplot(prediction_df, aes(x = `Price...`, y = distance_to_nearest_attraction, color = Deal.Quality)) +
  geom_point(alpha = 0.5) +
  labs(title = "Price vs. Distance to Nearest Attraction by Deal Quality", x = "Price", y = "Distance to Nearest Attraction") +
  theme_minimal() +
  scale_color_manual(values = c("Good" = "green", "Bad" = "red", "Unknown" = "blue"))
```

```{r}
ggplot(prediction_df, aes(x = `room.and.type`, fill = Deal.Quality)) +
  geom_bar(position = "fill") +
  labs(title = "Deal Quality Distribution by Room Type", x = "Room Type", y = "Proportion") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Completed Steps:
-Data Preparation and Cleaning: We've loaded the predictionChallenge.csv and Attractions.csv, ensuring that the data is clean and ready for analysis by handling NA values and verifying column names.
-Derived Attributes: We've discussed creating derived attributes such as distance_to_nearest_attraction and had plans for active_months (although we didn't explicitly calculate it due to column availability). These are examples of derived attributes that could help in modeling deal quality.
-Exploratory Data Analysis (EDA): We've created plots to visualize relationships in the data, such as the distribution of distance_to_nearest_attraction by deal quality and price versus distance to attractions. This helps in understanding the data and identifying potential patterns.

Remaining or Unclear Aspects:
-Comprehensive Feature Engineering: Depending on the prompt's requirement for "sub-patterns" and derived attributes, there might be additional features worth exploring. For instance, interactions between variables (e.g., price and location, type of room and review scores) or more complex derived attributes that could influence deal quality.
-Model Building and Evaluation: While we discussed aspects of modeling, we haven't explicitly built a predictive model to classify listings into "Good", "Bad", or "Unknown" deal qualities. Based on the prompt, achieving 100% accuracy with a model would indicate finding the correct prediction model. This step involves selecting appropriate modeling techniques, training the model(s), and evaluating their performance.
-Refinement and Validation: Given the goal of 100% accuracy, any initial model would likely require refinement. This could involve further feature engineering, model parameter tuning, or exploring different modeling approaches. Validation of the model's performance on unseen data is also crucial to ensure its reliability.

Next Steps to Fully Address the Prompt:
-Further Feature Engineering: Explore and create additional derived attributes that could capture the nuances and "sub-patterns" in the data. This may involve more detailed analysis of the existing features or integrating external data (e.g., from Attractions.csv).
-Model Development: Start with simple models to establish a baseline, then experiment with more complex models as needed. Given the categorical nature of the target variable, algorithms like logistic regression, decision trees, random forests, or gradient boosting could be appropriate.
-Evaluation and Refinement: Use cross-validation to evaluate model performance, focusing on accuracy given the prompt's requirement. Adjust the model based on performance metrics, and consider techniques like feature selection or hyper parameter tuning to improve accuracy.
-Validation: Once a model achieves high accuracy on the training data, validate it on a separate test set (if available) to ensure it generalizes well to new, unseen data.

```{r}
library(caret)
set.seed(123) # For reproducibility

# Assuming 'prediction_df' is your prepared dataframe
# Splitting the data into training (80%) and testing (20%) sets
index <- createDataPartition(prediction_df$Deal.Quality, p = 0.8, list = FALSE)
train_data <- prediction_df[index, ]
test_data <- prediction_df[-index, ]

# Convert categorical variables to factors
train_data$neighbourhood.group <- as.factor(train_data$neighbourhood.group)
train_data$room.and.type <- as.factor(train_data$room.and.type)
test_data$neighbourhood.group <- as.factor(test_data$neighbourhood.group)
test_data$room.and.type <- as.factor(test_data$room.and.type)

# Ensure 'Price...' is treated as numeric (adjust column name as necessary)
train_data$`Price...` <- as.numeric(as.character(train_data$`Price...`))
test_data$`Price...` <- as.numeric(as.character(test_data$`Price...`))
```

```{r}
## Model 1: Multinomial Logistic Regression
library(nnet)

# Adjusting the formula to include specific predictors
multinom_model <- multinom(Deal.Quality ~ number_of_reviews + reviews_per_month + `Price...` + mean_review_score + distance_to_nearest_attraction + neighbourhood.group + room.and.type, data = train_data)

# Making predictions on the test set
predictions_lr <- predict(multinom_model, newdata = test_data)

# Evaluating model performance
confusionMatrix(as.factor(predictions_lr), as.factor(test_data$Deal.Quality))
```

Multinomial Logistic Regression Model
-Accuracy: Approximately 71.63%, which is above the No Information Rate (NIR) of 49.7%. This indicates that the model has learned patterns from the data that are significantly better than random guessing.
-Class Sensitivity: Varies across classes, with "Unknown" having the highest sensitivity (79.24%). This suggests the model is best at identifying the "Unknown" deal quality, followed by "Good" and then "Bad".
-Balanced Accuracy: Shows variation across classes, indicating differing levels of performance in correctly predicting each class.

```{r}
## Model 2: Random Forest
library(randomForest)

# Removing rows with any NA values in the specified columns
train_data_clean <- na.omit(train_data, cols = c("number_of_reviews", "reviews_per_month", "`Price...`", "mean_review_score", "distance_to_nearest_attraction", "neighbourhood.group", "room.and.type"))

# Example of simple imputation for a numeric column
train_data$reviews_per_month[is.na(train_data$reviews_per_month)] <- mean(train_data$reviews_per_month, na.rm = TRUE)

# For categorical variables, replace NA with the most common category
train_data$neighbourhood.group[is.na(train_data$neighbourhood.group)] <- names(which.max(table(train_data$neighbourhood.group)))

# Ensure `Price...` is handled correctly, assuming it might need conversion from character to numeric
train_data$`Price...` <- as.numeric(as.character(train_data$`Price...`))

# Assuming missing values have been handled in `train_data_clean`
rf_model <- randomForest(as.factor(Deal.Quality) ~ number_of_reviews + reviews_per_month + `Price...` + mean_review_score + distance_to_nearest_attraction + neighbourhood.group + room.and.type, data = train_data_clean, ntree = 100)

rf_model
```
Random Forest Model
-Out-of-Bag (OOB) Error Rate: 8.72%, which suggests a high level of accuracy in classification across the three classes. The OOB error rate is a robust estimate of model performance and indicates that the random forest model performs quite well.
-Confusion Matrix: Shows a strong performance across all classes, with particularly high accuracy in predicting "Good" deals and "Unknown" deal quality. The "Bad" class has a slightly higher class error rate compared to the other two classes.
-Variable Importance: While not explicitly shown in your output, the randomForest model can provide insights into which variables are most important for predicting "Deal Quality". You can use the importance() function on your rf_model object to explore this.

```{r}
# Get feature importance
importance <- importance(rf_model)
# Plot feature importance
varImpPlot(rf_model)
```
Understanding which features are most important can help in interpreting the model and potentially in refining the model by focusing on the most influential predictors.

```{r}
library(caret)

# Setting up train control for cross-validation
train_control <- trainControl(method="cv", number=10)

# Tuning model
tuned_rf_model <- train(
  as.factor(Deal.Quality) ~ number_of_reviews + reviews_per_month + `Price...` + mean_review_score + distance_to_nearest_attraction + neighbourhood.group + room.and.type, 
  data = train_data_clean, 
  method = "rf",
  ntree = 100,
  trControl = train_control,
  tuneLength = 5 # Adjust based on computational resources and desired granularity
)

# Check the best tuning parameters
print(tuned_rf_model$bestTune)
```

Model tuning involves adjusting the model's hyperparameters to improve performance. For the Random Forest model, key parameters include ntree (number of trees) and mtry (number of variables tried at each split). For logistic regression, regularization might be considered, but it's more straightforward with models like glmnet rather than multinom from nnet.

Random Forest Tuning
We'll use the train function from the caret package to tune the Random Forest model. This example demonstrates tuning mtry, the number of variables considered at each split.

```{r}
# Viewing cross-validation results
results <- tuned_rf_model$results
print(results)
```
Cross-validation is used to assess how the outcomes of a statistical analysis will generalize to an independent dataset. It's already incorporated in the tuning process above via trainControl with method="cv" for the Random Forest model. For a more detailed assessment, you could perform additional cross-validation analyses or explore results across the folds.

```{r}
# Similar preprocessing for test_data as done for train_data before model training
test_data$reviews_per_month[is.na(test_data$reviews_per_month)] <- mean(train_data$reviews_per_month, na.rm = TRUE) # Use train_data mean for consistency
test_data$neighbourhood.group[is.na(test_data$neighbourhood.group)] <- names(which.max(table(train_data$neighbourhood.group))) # Use most common category from train_data

# Ensure factor levels in test_data match those in train_data for categorical predictors
test_data$neighbourhood.group <- factor(test_data$neighbourhood.group, levels = levels(train_data$neighbourhood.group))
test_data$room.and.type <- factor(test_data$room.and.type, levels = levels(train_data$room.and.type))

# Generate predictions for test_data using the trained random forest model
predictions_test <- predict(rf_model, newdata = test_data)

# Calculate accuracy
correct_predictions_test <- sum(predictions_test == test_data$Deal.Quality)
total_predictions_test <- length(predictions_test)
accuracy_test <- correct_predictions_test / total_predictions_test

# Print the accuracy
print(paste("Test Accuracy:", accuracy_test))
```
Through comprehensive data analysis and rigorous model evaluation, a predictive framework was successfully developed to classify the "Deal Quality" of Airbnb listings with an outstanding accuracy of approximately 90.91%. This remarkable achievement underscores the model's capability to effectively unravel the complex patterns embedded within the dataset. At the heart of this success were the ingeniously engineered features, particularly 'distance_to_nearest_attraction', as well as essential variables such as 'number_of_reviews', 'reviews_per_month', 'Price...', and 'mean_review_score', complemented by categorical variables like 'neighbourhood.group' and 'room.and.type'.

The deployment of the Random Forest model, celebrated for its robustness in managing intricate data relationships and averting overfitting, was skillfully executed to decipher the dataset's complexities. The standout performance of the model accentuated the significance of 'distance_to_nearest_attraction', corroborating the theory that the vicinity to attractions markedly influences a listing's perceived value. This revelation is in harmony with the intuitive understanding of the rental market, where the interplay between location and price crucially influences consumer perceptions and choices.

The methodical journey from data preprocessing and feature engineering to the culminating modeling phases was meticulously orchestrated to guarantee a comprehensive grasp and examination of the dataset. Although the elusive goal of 100% accuracy was not met, the model's exceptional accuracy is a testament to the judicious selection of features and the proficiency of the Random Forest algorithm in capturing the subtle dynamics governing "Deal Quality".

This venture not only broadens our understanding of the elements that amplify a listing's attractiveness but also establishes a foundational framework for subsequent explorations into the Airbnb market. The well-documented process, spanning from initial data handling to advanced model training, presents a detailed blueprint for tackling similar predictive modeling challenges. This accomplishment not only marks a significant milestone in predictive analytics within the hospitality domain but also sets a precedent for leveraging data-driven insights to inform strategic decisions and enhance consumer experiences.